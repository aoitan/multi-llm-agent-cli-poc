### A/Bテストスクリプト要件定義書

#### 1. 目的

本スクリプトは、LLMエージェントが使用するプロンプトのA/Bテストを汎用的に実行することを目的とする。これにより、様々な評価観点からプロンプトの性能を比較し、その時々の課題に応じた最適なプロンプトの選定、または手動でのプロンプト改善のための知見を得ることを可能にする。

#### 2. テスト対象のプロンプト

*   **現状プロンプト (Base Prompt)**: `prompts/default_prompts.json` に定義されているプロンプトを使用する。
*   **実験プロンプト (Experimental Prompt)**:
    *   設定ファイル（例: `config/ab_test_config.json`）に、実験用プロンプトファイルのパスを指定する項目を追加する。
    *   この設定ファイルは、既存の`configLoader`で読み込み可能とする。
    *   実験用プロンプトファイルは、`prompts/`ディレクトリ配下に配置することを想定する。

#### 3. テストの実行設定

*   **評価LLMのモデル指定**:
    *   テストドライバーのスクリプトから参照する設定ファイル（例: `scripts/ab_test_config.py`のようなPythonファイル、または既存の`config/`配下のJSONファイル）で、評価に使用するLLMモデル（例: `llama3:8b`）を指定できるようにする。
    *   これは、ブラインドテストスクリプトの`prompts_config.py`のような形式を参考に、汎用的な設定方法を検討する。
*   **実行回数**: 各プロンプト（現状プロンプト、実験プロンプト）に対して、ユーザーが指定した回数だけLLMを実行できるようにする。

#### 4. 評価プロンプトの生成と評価観点

*   **評価観点**:
    *   評価LLMに指示するプロンプトは、汎用性を重視し、特定の評価基準に限定しない。
    *   ただし、人間および商用クラウド型LLMでの評価を考慮し、以下の観点を提案する。
        *   **関連性**: ユーザーの元のプロンプトに対して、生成された回答がどの程度適切で関連性が高いか。
        *   **網羅性**: 回答がユーザーの意図する情報をどの程度網羅しているか。
        *   **正確性**: 回答に含まれる情報が事実に基づき、正確であるか。
        *   **一貫性**: 回答全体を通して論理的な矛盾がないか。
        *   **簡潔性**: 不要な情報を含まず、簡潔にまとめられているか。
        *   **明確性**: 回答が曖昧でなく、理解しやすいか。
        *   **トーン/スタイル**: 期待されるトーンやスタイル（例: 丁寧語、専門的、創造的）に合致しているか。
        *   **A/B比較**: AとBのどちらの回答が総合的に優れているか、その理由。
        *   **改善提案**: 回答をさらに改善するための具体的な提案。
    *   これらの観点を評価LLMに提示し、自由記述形式での評価を促す。
*   **評価結果の形式**:
    *   Markdown形式のテンプレートを与え、評価LLMがそのテンプレートに沿って評価結果を生成するようにする。
    *   選択肢のある項目（例: 5段階評価）以外は、パラグラフ内はフリーフォーマットとする。

#### 5. 出力結果の保存

*   **保存先**: `eval/prompt_comparison/YYYYMMDDHHMM/` 配下に、以下の情報を保存する。
    *   各テストケースの入力プロンプト（ユーザープロンプト、現状プロンプト、実験プロンプト）
    *   現状プロンプトおよび実験プロンプトで生成されたLLMの回答
    *   評価LLMによる評価結果（Markdown形式）
    *   テスト実行時の設定情報（使用モデル、実行回数など）
*   **ファイル形式**:
    *   各LLMの回答はテキストファイル（例: `base_output_N.md`, `exp_output_N.md`）
    *   評価結果はMarkdownファイル（例: `evaluation_N.md`）
    *   設定情報やメタデータはJSONファイル（例: `metadata.json`）
*   **既存のブラインドテストスクリプトの出力パス変更**:
    *   `doc/blind_evaluation/` に出力されているブラインドテストの結果を、`eval/model_comparison/YYYYMMDD/` に出力するように変更する。