# multi-llm-agent-cli-poc

## プロジェクト概要

このプロジェクトは、Ollama API を利用して複数の大規模言語モデル (LLM) 間で対話（相談）を行う概念実証 (PoC) です。ユーザーが入力したプロンプトに対し、異なる役割を持つLLMが議論を深め、最終的な結論や重要なポイントを要約して提示することを目指します。

## 機能

- **Ollama API連携**: ローカルで動作するOllamaサーバーを通じてLLMと対話します。
- **LLM間相談ロジック**: 以下の役割を持つLLMが複数ターンにわたって対話を行います。
    - **思考者**: ユーザーのプロンプトに対して素の思考で回答します。
    - **批判的レビュアー**: 思考者の回答を批判的にレビューし、改善点を見つけます。
    - **指摘改善者**: レビュアーの指摘を参考に、自身の回答を改善します。
- **会話の要約**: 全ての対話が終了した後、会話内容と最初のユーザープロンプトを基に、最終的な要約を生成します。

## インストール

1.  **Node.jsとnpmのインストール**: 
    お使いのシステムにNode.jsとnpmがインストールされていることを確認してください。

2.  **リポジトリのクローン**: 
    ```bash
    git clone https://github.com/aoitan/multi-llm-agent-cli-poc.git
    cd multi-llm-agent-cli-poc
    ```

3.  **依存関係のインストール**: 
    ```bash
    npm install
    ```

4.  **Ollamaのセットアップ**: 
    Ollamaをインストールし、ローカルで実行していることを確認してください。また、使用したいLLMモデルをプルしておいてください。
    例: `ollama pull gemma3:1b`

## 使用方法

プロジェクトのルートディレクトリで以下のコマンドを実行します。

```bash
npm start "あなたのプロンプト" [モデル1の名前] [モデル2の名前]
```

-   `<あなたのプロンプト>`: LLMに議論させたい内容を入力します。
-   `[モデル1の名前]`: 思考者および指摘改善者として使用するOllamaモデルの名前です。省略した場合、デフォルトで`llama3`が使用されます。
-   `[モデル2の名前]`: 批判的レビュアーとして使用するOllamaモデルの名前です。省略した場合、デフォルトで`llama3`が使用されます。

**例:**

```bash
npm start "日本の少子高齢化問題について解決策を議論してください" gemma3:1b gemma3:1b
```

## プロジェクト構造

-   `src/index.ts`: CLIのエントリーポイント。ユーザーの入力を受け取り、相談ロジックを呼び出します。
-   `src/agent.ts`: LLM間の相談ロジックと役割分担を定義しています。
-   `src/ollamaApi.ts`: Ollama APIとの通信を行うためのラッパー関数を提供します。
-   `doc/development_plan.md`: 開発計画が記載されています。

## ライセンス

このプロジェクトはMITライセンスの下で公開されています。詳細については`LICENSE`ファイルを参照してください。
